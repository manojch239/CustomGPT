# Custom Large Language Model (LLM) â€“ ChatGPT Style

> ğŸš§ Work in Progress  
> A personal project exploring how Large Language Models (LLMs) like ChatGPT are built and trained.

## ğŸ§  Key Components

### âœ… Data Collection & Preprocessing
<!-- - Used **FineWeb** dataset principles for curating large-scale textual data -->
- Performed cleaning and normalization
- Planned for multilingual dataset support
- Used **Shakespear** dataset to train the model  

### ğŸ§© Tokenization
- Implemented a simple encoding , decoding on the dataset
- Explored **Byte Pair Encoding (BPE)** for vocabulary compression

### ğŸ§± Model Architecture
- Based on **GPT architecture (Decoder-only Transformer)**
- Explored self-attention, positional embeddings, and masked attention mechanisms

### ğŸ§ª Training Pipeline (Learning in progress)
- Supervised Fine-Tuning (SFT) using synthetic prompt-response pairs
- Planned to learn **RLHF (Reinforcement Learning from Human Feedback)** for alignment
- Experimenting in both **TensorFlow** and **PyTorch**

### ğŸ–¥ï¸ Inference
- Working on deploying a demo interface using a small GPT model (e.g., GPT2-small) via HuggingFace Transformers

## ğŸ› ï¸ Tech Stack
- Python, PyTorch, TensorFlow, HuggingFace Transformers
- Jupyter Notebooks
- Tokenizers (BPEmb, tiktoken)
- Google Colab & Local GPU (for training)

## ğŸ“Œ Goals
- Understand and replicate the key stages of LLM development
- Fine-tune models for downstream NLP tasks
- Build an interactive chatbot based on a fine-tuned LLM

---
